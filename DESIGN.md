# Design Document: Contract Intelligence API

This document outlines the design choices for the Contract Intelligence API assignment.

## 1. Architecture Diagram

**Conceptual Diagram:**

```
+-----------------+     +-----------------------+     +---------------------+
| User / Client   | --> | FastAPI Application   | --> | Google Vertex AI    |
| (e.g., curl)    |     | (Docker Container)    |     | (Gemini, Embeddings)|
+-----------------+     +-----------------------+     +---------------------+
                                    |
                                    | Stores/Retrieves
                                    | Full Text & Chunks
                                    V
                        +-----------------+
                        | ChromaDB        |
                        | (Docker Container)|
                        +-----------------+
```

**Description:**

* **User/Client**: Interacts with the API via HTTP requests (e.g., `curl`, Swagger UI).
* **FastAPI Application**: The core backend service running in a Docker container. It handles:
    * API routing (using FastAPI).
    * PDF parsing (using PyMuPDF).
    * Text chunking (using LangChain `RecursiveCharacterTextSplitter`).
    * Interaction with Google Vertex AI (using LangChain integrations) for embeddings, extraction, Q&A, and auditing.
    * Interaction with ChromaDB (using the `chromadb` client) for storing/retrieving full documents and text chunks/vectors.
    * Serving API endpoints (`/ingest`, `/extract`, `/ask`, `/audit`, `/ask/stream`, `/healthz`, `/metrics`, `/docs`).
* **Google Vertex AI**: External managed service providing the Generative AI models (Gemini 2.5 Flash for generation, Text Embedding 004 for RAG) accessed via API calls. Authentication is handled via a service account key.
* **ChromaDB**: Vector database running in a separate Docker container. It stores:
    * A collection (`contracts`) holding the full text of ingested documents, indexed by `document_id`.
    * A collection (`contract_chunks`) holding text chunks and their corresponding vector embeddings, indexed by `chunk_id` (`{document_id}_{chunk_number}`) and containing metadata (`document_id`, `filename`, `chunk_number`).

## 2. Data Model

**ChromaDB Collections:**

1.  **`contracts` Collection:**
    * **Purpose**: Stores the full text of each document primarily for the `/extract` and `/audit` endpoints.
    * **ID**: `document_id` (UUID generated during ingest).
    * **Document**: Full extracted text of the PDF.
    * **Metadata**: `{"filename": str, "doc_id": str}` (Storing `doc_id` in metadata allows filtering if needed, though we primarily use the main ID for retrieval here).

2.  **`contract_chunks` Collection:**
    * **Purpose**: Stores text chunks and their embeddings for efficient similarity search in the RAG process (`/ask`, `/ask/stream`).
    * **ID**: `chunk_id` (string formatted as `{document_id}_{chunk_number}`).
    * **Document**: The text content of the chunk.
    * **Embedding**: Vector embedding of the document text (generated by Vertex AI `text-embedding-004`).
    * **Metadata**: `{"document_id": str, "filename": str, "chunk_number": int}`. This metadata is crucial for providing citations.

*(No relational database is used in this design for simplicity; ChromaDB handles the necessary storage).*

## 3. Chunking Rationale

* **Strategy**: `RecursiveCharacterTextSplitter` from LangChain.
* **Parameters**: `chunk_size=1000`, `chunk_overlap=100`.
* **Rationale**:
    * `RecursiveCharacterTextSplitter` is chosen as a standard LangChain method. It attempts to split text based on common separators (`\n\n`, `\n`, ` `, ``) to keep related content (like paragraphs) together where possible.
    * `chunk_size=1000` is a common default that balances providing enough context for the embedding model and the downstream LLM without exceeding context window limits or becoming too computationally expensive.
    * `chunk_overlap=100` helps maintain context between chunks. If a sentence or idea spans across the boundary of two chunks, the overlap ensures that this context isn't completely lost during retrieval, improving the chances of finding relevant information.
    * These parameters are general defaults and could be further optimized based on the specific structure of typical contracts and the chosen embedding/LLM models.

## 4. Fallback Behavior

* **LLM Failures**: If calls to Vertex AI (for `/extract`, `/ask`, `/audit`, or embeddings) fail (e.g., API errors, network issues, quota limits), the respective endpoint currently returns a 500 Internal Server Error (via `HTTPException`) or includes an error message in the response (like in `/audit`'s `except` block). A production system would implement more robust error handling, potentially including retries with exponential backoff.
* **Rule Engine Fallback (Not Implemented)**: The assignment mentions demonstrating a "rule engine fallback". This version **does not include** an explicit rule-based (non-LLM) fallback for extraction or auditing.
    * **Potential Implementation**: A fallback could involve using regular expressions (regex) or keyword matching to find specific fields (`/extract`) or flag predefined risky phrases (`/audit`) if the LLM fails or if a faster/cheaper deterministic check is desired. This would require adding separate logic branches within the endpoints.
* **Database Failures**: If ChromaDB is unavailable during startup, the `lifespan` function attempts retries before failing. If it fails during an API call (e.g., `/ingest`, `/extract`, `/ask`), the endpoint's `try...except` block catches the error and returns an appropriate error response or raises an `HTTPException`.

## 5. Security Notes

* **Authentication**: The primary authentication mechanism is the Google Cloud Service Account key (`gcp-service-account.json`) used to access Vertex AI. This key is mounted into the Docker container via `docker-compose.yml` and accessed through the `GOOGLE_APPLICATION_CREDENTIALS` environment variable. The key file itself should be kept confidential and is included in `.gitignore`.
* **API Access**: The FastAPI endpoints themselves are **not** currently protected by any authentication or authorization layer. In a real-world scenario, methods like API keys (via headers), OAuth2, or JWT would be implemented to control access.
* **Input Validation**: FastAPI automatically validates incoming request bodies and query parameters against the Pydantic models, preventing many common injection-type vulnerabilities. File uploads are handled using FastAPI's `UploadFile`.
* **Secrets Management**: The GCP Project ID is stored in a `.env` file, loaded by Docker Compose. The service account key is stored as a JSON file. These should not be committed to version control.
* **Container Security**: Using official base images (Python, ChromaDB) is generally good practice. Regular updates and vulnerability scanning would be needed in production.
* **PII/Confidential Data**: The system processes contract PDFs which may contain sensitive information.
    * **Logging**: The assignment requires logs redacting PII. This **is not currently implemented** in the code. Logic would need to be added to identify and mask sensitive data (names, addresses, financial details) before logging request/response data or internal variables.
    * **Data Storage**: Data (full text, chunks) is stored locally within the ChromaDB Docker volume. Encryption at rest and access controls for the database would be critical in production.